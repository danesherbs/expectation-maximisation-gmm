%% Inputs:%     - Data matrix (m x d) of m d-dimensional data points.%     - Number of clusters k.%% Outputs:%     - mu (d x k) matrix of means of each cluster%     - sigma (d x d x k) matrix of covariances for each cluster%function [mu, sigma] = em(data, k)    % Extract m and d  [m, d] = size(data);    % Initialise parameters of model (to be estimated with EM)  phi = ones(k, 1) / k;  mu = zeros(d, k);  sigma = zeros(d, d, k);  for i = 1:k    mu(i, :) = 10 * rand(1, d);  % random mu's    L = tril(rand(d, d));  % sample lower tri matrix    sigma(:, :, i) = L * L';  % make positive definite matrix via Cholesky decomp.  endfor  % E-step (guess labels of data points)  w = zeros(m, k);  for i = 1:m    % Calculate normalising constant    norm_constant = 0;    for l = 1:k      norm_constant += mvnpdf(X(i, :), mu(l, :), sigma(:, :, l));    endfor    % Calculate probability of being in each cluster for current data point    for j = 1:k      w(i, j) = mvnpdf(X(i, :), mu(j, :), sigma(:, :, j)) / norm_constant;    endfor  endfor                                                             % M-step (update values of parameters)  phi = ones(k, 1) / k;  mu = zeros(k, d);  sigma = zeros(d, d, k);  for j = 1:k    % Update mixing coefficient of cluster    phi(j) = 1/m * sum(w(:, j));    % Update mean and covariance of cluster    mu(j, :) = w(:, j)' * X(:, :) / sum(w(:, j));    for i = 1:m      sigma(:, :, j) += w(i, j) * (X(i, :) - mu(j, :)) * (X(i, :) - mu(j, :))' / sum(w(:, j));    endfor  endfor  
endfunction
pkg load statistics% Define number of data points and dimensionsm = 1000;  % number of data pointsd = 2;  % dimensions% Sample points from GMMphi = [0.25, 0.25, 0.5];k = length(phi);[xs, ys, ~] = sample_gmm(phi, m);X = [xs', ys'];  % (m x d) matrix% Initialise mu and sigmamu = zeros(k, d);sigma = zeros(d, d, k);% Randomly initialise mu's and sigma'sfor i = 1:k  mu(i, :) = 10 * rand(1, d);  L = tril(rand(d, d));  % sample lower tri matrix  sigma(:, :, i) = L * L';  % make positive definite matrix via Cholesky decomp.endfor% E-step (guess labels of data points)w = zeros(m, k);for i = 1:m  % Calculate normalising constant  norm_constant = 0;  for l = 1:k    norm_constant += mvnpdf(X(i, :), mu(l, :), sigma(:, :, l));  endfor  % Calculate probability of being in each cluster for current data point  for j = 1:k    w(i, j) = mvnpdf(X(i, :), mu(j, :), sigma(:, :, j)) / norm_constant;  endforendfor                                                         % M-step (update values of parameters)phi = ones(k, 1) / k;mu = zeros(k, d);sigma = zeros(d, d, k);for j = 1:k  % Update mixing coefficient of cluster  phi(j) = 1/m * sum(w(:, j));  % Update mean and covariance of cluster  mu(j, :) = w(:, j)' * X(:, :) / sum(w(:, j));  for i = 1:m    sigma(:, :, j) += w(i, j) * (X(i, :) - mu(j, :)) * (X(i, :) - mu(j, :))' / sum(w(:, j));  endforendfor